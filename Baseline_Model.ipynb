{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF7UGvkRCMZe",
        "outputId": "6629d73d-7607-4bf3-e3ff-8a77277374e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tf_keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZBBwsvTOCSdf",
        "outputId": "49f7aa5d-0e1e-463b-a52c-e7c9b9ed2bd0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 MB\u001b[0m \u001b[31m765.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.4.1 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "5b153b44e4cf4cf6a8c5310021a595c4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKiSU7JUDVnH",
        "outputId": "8ecc26ef-2913-4784-fc00-88e7cd08a7ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "每10个epoch save一遍model, save output, 用val generate一遍output, 最后跑完再用test data generate一遍（这个我觉得可以等我们觉得model效果ok了以后再跑，防止他memorize test data）--》 用作对比for qualitative and quantitative data\n",
        "\n",
        "再对d loss和g loss都画个图\n",
        "\n",
        "解决一下生成出来时黑白的问题\n",
        "\n",
        "train func也得改改\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PgHcABFtbjf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = '/content/drive/MyDrive/Colab Notebooks/Baseline Model'\n",
        "\n",
        "photo_train_dir = main_dir + '/photos_train_student_removeBG'\n",
        "photo_test_dir = main_dir + '/photos_test_student_removeBG'\n",
        "photo_val_dir = main_dir + '/photos_val_student_removeBG'\n",
        "\n",
        "sketch_train_dir = main_dir + '/sketch_train_studentSaved'\n",
        "sketch_test_dir = main_dir + '/sketch_test_studentSaved'\n",
        "sketch_val_dir = main_dir + '/sketch_val_studentSaved'"
      ],
      "metadata": {
        "id": "EiBjwfvEDmXJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageFolderDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('jpg', 'png', 'jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ],
      "metadata": {
        "id": "ccxURzm4RCn5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "photo_train_dataset = ImageFolderDataset(photo_train_dir, transform=transform)\n",
        "photo_test_dataset = ImageFolderDataset(photo_test_dir, transform=transform)\n",
        "photo_val_dataset = ImageFolderDataset(photo_val_dir, transform=transform)\n",
        "sketch_train_dataset = ImageFolderDataset(sketch_train_dir, transform=transform)\n",
        "sketch_test_dataset = ImageFolderDataset(sketch_test_dir, transform=transform)\n",
        "sketch_val_dataset = ImageFolderDataset(sketch_val_dir, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gxNjduT9RfQs",
        "outputId": "2b5c3856-75bd-44bc-b3a4-3c3822f40796"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'transforms' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f100700636df>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Define the Generator\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, features=64):\n",
        "        super(UNetGenerator, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            self.contract_block(in_channels, features, False),\n",
        "            self.contract_block(features, features * 2),\n",
        "            self.contract_block(features * 2, features * 4),\n",
        "            self.contract_block(features * 4, features * 8),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            self.expand_block(features * 8, features * 4),\n",
        "            self.expand_block(features * 4, features * 2),\n",
        "            self.expand_block(features * 2, features),\n",
        "            nn.ConvTranspose2d(features, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def contract_block(self, in_channels, out_channels, use_bn=True):\n",
        "        block = [nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)]\n",
        "        if use_bn:\n",
        "            block.append(nn.BatchNorm2d(out_channels))\n",
        "        block.append(nn.ReLU(inplace=True))\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "    def expand_block(self, in_channels, out_channels):\n",
        "        block = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "# Define the Discriminator\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels, features=64):\n",
        "        super(PatchDiscriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            self.discriminator_block(in_channels * 2, features, False),\n",
        "            self.discriminator_block(features, features * 2),\n",
        "            self.discriminator_block(features * 2, features * 4),\n",
        "            nn.Conv2d(features * 4, 1, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.model(torch.cat([x, y], dim=1))\n",
        "\n",
        "    def discriminator_block(self, in_channels, out_channels, use_bn=True):\n",
        "        block = [nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)]\n",
        "        if use_bn:\n",
        "            block.append(nn.BatchNorm2d(out_channels))\n",
        "        block.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "# Training Loop\n",
        "def train_gan(generator, discriminator, photo_dataloader, sketch_dataloader, num_epochs=200, lambda_l1=100):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    generator = generator.to(device)\n",
        "    discriminator = discriminator.to(device)\n",
        "\n",
        "    criterion_gan = nn.BCELoss()\n",
        "    criterion_l1 = nn.L1Loss()\n",
        "\n",
        "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "    output_dir = main_dir + '/output'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (photo_batch, sketch_batch) in enumerate(zip(photo_dataloader, sketch_dataloader)):\n",
        "            photo_batch = photo_batch.to(device)\n",
        "            sketch_batch = sketch_batch.to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            optimizer_d.zero_grad()\n",
        "            fake_sketch = generator(photo_batch)\n",
        "            real_validity = discriminator(photo_batch, sketch_batch)\n",
        "            fake_validity = discriminator(photo_batch, fake_sketch.detach())\n",
        "\n",
        "            real_loss = criterion_gan(real_validity, torch.ones_like(real_validity, device=device))\n",
        "            fake_loss = criterion_gan(fake_validity, torch.zeros_like(fake_validity, device=device))\n",
        "            d_loss = (real_loss + fake_loss)\n",
        "            d_loss.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # Train Generator\n",
        "            optimizer_g.zero_grad()\n",
        "            fake_validity = discriminator(photo_batch, fake_sketch)\n",
        "            gan_loss = criterion_gan(fake_validity, torch.ones_like(fake_validity, device=device))\n",
        "            l1_loss = criterion_l1(fake_sketch, sketch_batch)\n",
        "            g_loss = gan_loss + lambda_l1 * l1_loss\n",
        "            g_loss.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Epoch [{epoch}/{num_epochs}] Batch {i}/{len(photo_dataloader)} \\\n",
        "                      Loss D: {d_loss.item()}, loss G: {g_loss.item()}\")\n",
        "\n",
        "        # Save some generated images for visualization\n",
        "\n",
        "        save_image(fake_sketch[:25], f'{output_dir}/{epoch}_generated.png', nrow=5, normalize=True)\n",
        "\n",
        "# Load datasets\n",
        "photo_train_dataset = ImageFolderDataset(photo_train_dir, transform=transform)\n",
        "photo_test_dataset = ImageFolderDataset(photo_test_dir, transform=transform)\n",
        "photo_val_dataset = ImageFolderDataset(photo_val_dir, transform=transform)\n",
        "sketch_train_dataset = ImageFolderDataset(sketch_train_dir, transform=transform)\n",
        "sketch_test_dataset = ImageFolderDataset(sketch_test_dir, transform=transform)\n",
        "sketch_val_dataset = ImageFolderDataset(sketch_val_dir, transform=transform)\n",
        "\n",
        "photo_train_loader = DataLoader(photo_train_dataset, batch_size=32, shuffle=True)\n",
        "sketch_train_loader = DataLoader(sketch_train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize models\n",
        "generator = UNetGenerator(in_channels=3, out_channels=3)\n",
        "discriminator = PatchDiscriminator(in_channels=3)\n",
        "\n",
        "# Train the model\n",
        "train_gan(generator, discriminator, photo_train_loader, sketch_train_loader, num_epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPiSZkp1BxXX",
        "outputId": "7ad94d10-d92f-4fcf-f342-fd9e8dab3235"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/200] Batch 0/6                       Loss D: 0.7071510553359985, loss G: 86.29266357421875\n",
            "Epoch [1/200] Batch 0/6                       Loss D: 0.5914514660835266, loss G: 58.717079162597656\n",
            "Epoch [2/200] Batch 0/6                       Loss D: 0.4685845375061035, loss G: 48.92205810546875\n",
            "Epoch [3/200] Batch 0/6                       Loss D: 0.38676339387893677, loss G: 44.62611770629883\n",
            "Epoch [4/200] Batch 0/6                       Loss D: 0.31247466802597046, loss G: 40.675174713134766\n",
            "Epoch [5/200] Batch 0/6                       Loss D: 0.2677774429321289, loss G: 38.41364669799805\n",
            "Epoch [6/200] Batch 0/6                       Loss D: 0.23058564960956573, loss G: 35.30905532836914\n",
            "Epoch [7/200] Batch 0/6                       Loss D: 0.23361793160438538, loss G: 34.387123107910156\n",
            "Epoch [8/200] Batch 0/6                       Loss D: 0.1881713569164276, loss G: 31.21179962158203\n",
            "Epoch [9/200] Batch 0/6                       Loss D: 0.18105852603912354, loss G: 28.976058959960938\n",
            "Epoch [10/200] Batch 0/6                       Loss D: 0.15109984576702118, loss G: 27.8568058013916\n",
            "Epoch [11/200] Batch 0/6                       Loss D: 0.16620036959648132, loss G: 28.430875778198242\n",
            "Epoch [12/200] Batch 0/6                       Loss D: 0.11533595621585846, loss G: 25.800369262695312\n",
            "Epoch [13/200] Batch 0/6                       Loss D: 0.1304389238357544, loss G: 25.411518096923828\n",
            "Epoch [14/200] Batch 0/6                       Loss D: 0.11412039399147034, loss G: 24.044179916381836\n",
            "Epoch [15/200] Batch 0/6                       Loss D: 0.09203926473855972, loss G: 24.872085571289062\n",
            "Epoch [16/200] Batch 0/6                       Loss D: 0.0700860321521759, loss G: 23.78449249267578\n",
            "Epoch [17/200] Batch 0/6                       Loss D: 0.08382625877857208, loss G: 23.023164749145508\n",
            "Epoch [18/200] Batch 0/6                       Loss D: 0.0718487799167633, loss G: 21.549036026000977\n",
            "Epoch [19/200] Batch 0/6                       Loss D: 0.0735865980386734, loss G: 22.571575164794922\n",
            "Epoch [20/200] Batch 0/6                       Loss D: 0.05476465821266174, loss G: 22.467449188232422\n",
            "Epoch [21/200] Batch 0/6                       Loss D: 0.04578835889697075, loss G: 20.97970199584961\n",
            "Epoch [22/200] Batch 0/6                       Loss D: 0.07149659097194672, loss G: 22.54288673400879\n",
            "Epoch [23/200] Batch 0/6                       Loss D: 0.46809226274490356, loss G: 19.967628479003906\n",
            "Epoch [24/200] Batch 0/6                       Loss D: 0.10910424590110779, loss G: 19.13295555114746\n",
            "Epoch [25/200] Batch 0/6                       Loss D: 0.0796564519405365, loss G: 18.51032066345215\n",
            "Epoch [26/200] Batch 0/6                       Loss D: 0.10099861025810242, loss G: 17.697877883911133\n",
            "Epoch [27/200] Batch 0/6                       Loss D: 0.06412352621555328, loss G: 18.97315216064453\n",
            "Epoch [28/200] Batch 0/6                       Loss D: 0.06942300498485565, loss G: 18.294069290161133\n",
            "Epoch [29/200] Batch 0/6                       Loss D: 0.06178518384695053, loss G: 18.86102867126465\n",
            "Epoch [30/200] Batch 0/6                       Loss D: 0.06255510449409485, loss G: 17.757957458496094\n",
            "Epoch [31/200] Batch 0/6                       Loss D: 0.04358631372451782, loss G: 20.04290199279785\n",
            "Epoch [32/200] Batch 0/6                       Loss D: 0.04165806621313095, loss G: 17.214603424072266\n",
            "Epoch [33/200] Batch 0/6                       Loss D: 0.04369930922985077, loss G: 18.283349990844727\n",
            "Epoch [34/200] Batch 0/6                       Loss D: 0.07673415541648865, loss G: 18.002229690551758\n",
            "Epoch [35/200] Batch 0/6                       Loss D: 0.05522483214735985, loss G: 18.425199508666992\n",
            "Epoch [36/200] Batch 0/6                       Loss D: 0.05755306035280228, loss G: 16.944541931152344\n",
            "Epoch [37/200] Batch 0/6                       Loss D: 0.044583335518836975, loss G: 17.987289428710938\n",
            "Epoch [38/200] Batch 0/6                       Loss D: 0.04740186780691147, loss G: 17.53662109375\n",
            "Epoch [39/200] Batch 0/6                       Loss D: 0.03973132744431496, loss G: 16.919755935668945\n",
            "Epoch [40/200] Batch 0/6                       Loss D: 0.03529171645641327, loss G: 17.14323616027832\n",
            "Epoch [41/200] Batch 0/6                       Loss D: 0.02960927039384842, loss G: 16.770883560180664\n",
            "Epoch [42/200] Batch 0/6                       Loss D: 0.03075059875845909, loss G: 17.076581954956055\n",
            "Epoch [43/200] Batch 0/6                       Loss D: 0.02966262772679329, loss G: 17.435489654541016\n",
            "Epoch [44/200] Batch 0/6                       Loss D: 0.030117053538560867, loss G: 17.75545883178711\n",
            "Epoch [45/200] Batch 0/6                       Loss D: 0.027147581800818443, loss G: 18.244356155395508\n",
            "Epoch [46/200] Batch 0/6                       Loss D: 0.029387857764959335, loss G: 17.816181182861328\n",
            "Epoch [47/200] Batch 0/6                       Loss D: 0.027430865913629532, loss G: 17.325340270996094\n",
            "Epoch [48/200] Batch 0/6                       Loss D: 0.036180444061756134, loss G: 16.204330444335938\n",
            "Epoch [49/200] Batch 0/6                       Loss D: 0.08942263573408127, loss G: 16.36469078063965\n",
            "Epoch [50/200] Batch 0/6                       Loss D: 0.8726866245269775, loss G: 13.360145568847656\n",
            "Epoch [51/200] Batch 0/6                       Loss D: 0.568287193775177, loss G: 13.007198333740234\n",
            "Epoch [52/200] Batch 0/6                       Loss D: 0.5232583284378052, loss G: 12.548982620239258\n",
            "Epoch [53/200] Batch 0/6                       Loss D: 0.5565987825393677, loss G: 13.082547187805176\n",
            "Epoch [54/200] Batch 0/6                       Loss D: 0.4272695779800415, loss G: 13.853982925415039\n",
            "Epoch [55/200] Batch 0/6                       Loss D: 0.3329771161079407, loss G: 13.38644790649414\n",
            "Epoch [56/200] Batch 0/6                       Loss D: 0.5344336628913879, loss G: 14.3237943649292\n",
            "Epoch [57/200] Batch 0/6                       Loss D: 0.3953557014465332, loss G: 13.31103229522705\n",
            "Epoch [58/200] Batch 0/6                       Loss D: 0.48949384689331055, loss G: 13.84278678894043\n",
            "Epoch [59/200] Batch 0/6                       Loss D: 0.4371674060821533, loss G: 12.268110275268555\n",
            "Epoch [60/200] Batch 0/6                       Loss D: 0.4241940677165985, loss G: 13.551817893981934\n",
            "Epoch [61/200] Batch 0/6                       Loss D: 0.4543880224227905, loss G: 12.42466926574707\n",
            "Epoch [62/200] Batch 0/6                       Loss D: 0.6215308904647827, loss G: 14.097936630249023\n",
            "Epoch [63/200] Batch 0/6                       Loss D: 0.48542845249176025, loss G: 12.002628326416016\n",
            "Epoch [64/200] Batch 0/6                       Loss D: 0.5548286437988281, loss G: 12.038248062133789\n",
            "Epoch [65/200] Batch 0/6                       Loss D: 0.4184854030609131, loss G: 13.661462783813477\n",
            "Epoch [66/200] Batch 0/6                       Loss D: 0.4843555688858032, loss G: 14.026512145996094\n",
            "Epoch [67/200] Batch 0/6                       Loss D: 0.5402799844741821, loss G: 11.950243949890137\n",
            "Epoch [68/200] Batch 0/6                       Loss D: 0.40816983580589294, loss G: 12.809983253479004\n",
            "Epoch [69/200] Batch 0/6                       Loss D: 0.43059664964675903, loss G: 12.69342041015625\n",
            "Epoch [70/200] Batch 0/6                       Loss D: 0.5041120052337646, loss G: 12.418967247009277\n",
            "Epoch [71/200] Batch 0/6                       Loss D: 0.3584067225456238, loss G: 12.511094093322754\n",
            "Epoch [72/200] Batch 0/6                       Loss D: 0.6312037110328674, loss G: 12.177885055541992\n",
            "Epoch [73/200] Batch 0/6                       Loss D: 0.40438663959503174, loss G: 12.792734146118164\n",
            "Epoch [74/200] Batch 0/6                       Loss D: 0.6654686331748962, loss G: 12.427286148071289\n",
            "Epoch [75/200] Batch 0/6                       Loss D: 0.48495620489120483, loss G: 11.803552627563477\n",
            "Epoch [76/200] Batch 0/6                       Loss D: 0.5008227229118347, loss G: 13.042797088623047\n",
            "Epoch [77/200] Batch 0/6                       Loss D: 0.5411656498908997, loss G: 11.509244918823242\n",
            "Epoch [78/200] Batch 0/6                       Loss D: 0.6210119128227234, loss G: 11.763306617736816\n",
            "Epoch [79/200] Batch 0/6                       Loss D: 0.5007289052009583, loss G: 13.161945343017578\n",
            "Epoch [80/200] Batch 0/6                       Loss D: 0.29986703395843506, loss G: 12.506110191345215\n",
            "Epoch [81/200] Batch 0/6                       Loss D: 0.554509162902832, loss G: 12.4666109085083\n",
            "Epoch [82/200] Batch 0/6                       Loss D: 0.5138417482376099, loss G: 11.499776840209961\n",
            "Epoch [83/200] Batch 0/6                       Loss D: 0.4438299536705017, loss G: 12.796307563781738\n",
            "Epoch [84/200] Batch 0/6                       Loss D: 0.5586057901382446, loss G: 12.943387031555176\n",
            "Epoch [85/200] Batch 0/6                       Loss D: 0.5761909484863281, loss G: 12.77160358428955\n",
            "Epoch [86/200] Batch 0/6                       Loss D: 0.6620409488677979, loss G: 12.982841491699219\n",
            "Epoch [87/200] Batch 0/6                       Loss D: 0.8581462502479553, loss G: 12.315120697021484\n",
            "Epoch [88/200] Batch 0/6                       Loss D: 0.5232954621315002, loss G: 12.603177070617676\n",
            "Epoch [89/200] Batch 0/6                       Loss D: 0.6137316226959229, loss G: 12.267620086669922\n",
            "Epoch [90/200] Batch 0/6                       Loss D: 0.47115403413772583, loss G: 13.316926956176758\n",
            "Epoch [91/200] Batch 0/6                       Loss D: 0.5971450209617615, loss G: 12.14316177368164\n",
            "Epoch [92/200] Batch 0/6                       Loss D: 0.6136842370033264, loss G: 10.221380233764648\n",
            "Epoch [93/200] Batch 0/6                       Loss D: 0.4980059266090393, loss G: 13.004274368286133\n",
            "Epoch [94/200] Batch 0/6                       Loss D: 0.4367097318172455, loss G: 12.151612281799316\n",
            "Epoch [95/200] Batch 0/6                       Loss D: 0.444395512342453, loss G: 11.511006355285645\n",
            "Epoch [96/200] Batch 0/6                       Loss D: 0.6607409715652466, loss G: 13.052178382873535\n",
            "Epoch [97/200] Batch 0/6                       Loss D: 0.4341531991958618, loss G: 12.229175567626953\n",
            "Epoch [98/200] Batch 0/6                       Loss D: 0.3788990378379822, loss G: 11.31857967376709\n",
            "Epoch [99/200] Batch 0/6                       Loss D: 0.8608587980270386, loss G: 12.522090911865234\n",
            "Epoch [100/200] Batch 0/6                       Loss D: 0.5633774995803833, loss G: 12.820847511291504\n",
            "Epoch [101/200] Batch 0/6                       Loss D: 0.5415418148040771, loss G: 12.886720657348633\n",
            "Epoch [102/200] Batch 0/6                       Loss D: 0.40055030584335327, loss G: 12.751398086547852\n",
            "Epoch [103/200] Batch 0/6                       Loss D: 0.7282283306121826, loss G: 12.292927742004395\n",
            "Epoch [104/200] Batch 0/6                       Loss D: 0.4056144654750824, loss G: 12.381475448608398\n",
            "Epoch [105/200] Batch 0/6                       Loss D: 0.5032280087471008, loss G: 11.713068962097168\n",
            "Epoch [106/200] Batch 0/6                       Loss D: 0.3661791682243347, loss G: 12.309999465942383\n",
            "Epoch [107/200] Batch 0/6                       Loss D: 0.6875770092010498, loss G: 13.402688026428223\n",
            "Epoch [108/200] Batch 0/6                       Loss D: 0.538547933101654, loss G: 11.085275650024414\n",
            "Epoch [109/200] Batch 0/6                       Loss D: 0.5422242879867554, loss G: 13.662328720092773\n",
            "Epoch [110/200] Batch 0/6                       Loss D: 0.4731079936027527, loss G: 13.707767486572266\n",
            "Epoch [111/200] Batch 0/6                       Loss D: 0.3937187194824219, loss G: 11.492493629455566\n",
            "Epoch [112/200] Batch 0/6                       Loss D: 0.31025931239128113, loss G: 11.95844841003418\n",
            "Epoch [113/200] Batch 0/6                       Loss D: 0.369759738445282, loss G: 11.999899864196777\n",
            "Epoch [114/200] Batch 0/6                       Loss D: 0.33950889110565186, loss G: 12.457480430603027\n",
            "Epoch [115/200] Batch 0/6                       Loss D: 0.5778928995132446, loss G: 12.19015121459961\n",
            "Epoch [116/200] Batch 0/6                       Loss D: 0.35762399435043335, loss G: 11.621843338012695\n",
            "Epoch [117/200] Batch 0/6                       Loss D: 0.34796279668807983, loss G: 13.583556175231934\n",
            "Epoch [118/200] Batch 0/6                       Loss D: 0.43422234058380127, loss G: 11.535642623901367\n",
            "Epoch [119/200] Batch 0/6                       Loss D: 0.4436536431312561, loss G: 11.582534790039062\n",
            "Epoch [120/200] Batch 0/6                       Loss D: 0.45781534910202026, loss G: 12.296313285827637\n",
            "Epoch [121/200] Batch 0/6                       Loss D: 0.4318505525588989, loss G: 11.258255004882812\n",
            "Epoch [122/200] Batch 0/6                       Loss D: 0.5998300313949585, loss G: 12.34084701538086\n",
            "Epoch [123/200] Batch 0/6                       Loss D: 0.4980066120624542, loss G: 11.778804779052734\n",
            "Epoch [124/200] Batch 0/6                       Loss D: 0.4958711266517639, loss G: 14.325201034545898\n",
            "Epoch [125/200] Batch 0/6                       Loss D: 0.4224088788032532, loss G: 12.080643653869629\n",
            "Epoch [126/200] Batch 0/6                       Loss D: 0.3622334599494934, loss G: 11.726532936096191\n",
            "Epoch [127/200] Batch 0/6                       Loss D: 0.567229151725769, loss G: 11.633879661560059\n",
            "Epoch [128/200] Batch 0/6                       Loss D: 0.40957772731781006, loss G: 12.461183547973633\n",
            "Epoch [129/200] Batch 0/6                       Loss D: 0.4884277582168579, loss G: 11.851633071899414\n",
            "Epoch [130/200] Batch 0/6                       Loss D: 0.6824322938919067, loss G: 11.31920337677002\n",
            "Epoch [131/200] Batch 0/6                       Loss D: 0.4593238830566406, loss G: 12.44502067565918\n",
            "Epoch [132/200] Batch 0/6                       Loss D: 0.7201820015907288, loss G: 13.460338592529297\n",
            "Epoch [133/200] Batch 0/6                       Loss D: 0.5480421185493469, loss G: 12.489298820495605\n",
            "Epoch [134/200] Batch 0/6                       Loss D: 0.3326864242553711, loss G: 11.850600242614746\n",
            "Epoch [135/200] Batch 0/6                       Loss D: 0.3320332169532776, loss G: 11.700105667114258\n",
            "Epoch [136/200] Batch 0/6                       Loss D: 0.44569623470306396, loss G: 11.619224548339844\n",
            "Epoch [137/200] Batch 0/6                       Loss D: 0.31141382455825806, loss G: 11.242548942565918\n",
            "Epoch [138/200] Batch 0/6                       Loss D: 0.5552953481674194, loss G: 12.786603927612305\n",
            "Epoch [139/200] Batch 0/6                       Loss D: 0.43321701884269714, loss G: 11.874408721923828\n",
            "Epoch [140/200] Batch 0/6                       Loss D: 0.6826171875, loss G: 14.686281204223633\n",
            "Epoch [141/200] Batch 0/6                       Loss D: 0.38986432552337646, loss G: 11.878242492675781\n",
            "Epoch [142/200] Batch 0/6                       Loss D: 0.4747668504714966, loss G: 11.579277992248535\n",
            "Epoch [143/200] Batch 0/6                       Loss D: 0.4421354830265045, loss G: 10.987608909606934\n",
            "Epoch [144/200] Batch 0/6                       Loss D: 0.6814594268798828, loss G: 12.412419319152832\n",
            "Epoch [145/200] Batch 0/6                       Loss D: 0.46707725524902344, loss G: 11.501434326171875\n",
            "Epoch [146/200] Batch 0/6                       Loss D: 0.5811645984649658, loss G: 12.036124229431152\n",
            "Epoch [147/200] Batch 0/6                       Loss D: 0.46023088693618774, loss G: 14.089336395263672\n",
            "Epoch [148/200] Batch 0/6                       Loss D: 0.4912029504776001, loss G: 12.223066329956055\n",
            "Epoch [149/200] Batch 0/6                       Loss D: 0.8473426699638367, loss G: 11.788397789001465\n",
            "Epoch [150/200] Batch 0/6                       Loss D: 0.4393787384033203, loss G: 12.476018905639648\n",
            "Epoch [151/200] Batch 0/6                       Loss D: 0.5839664340019226, loss G: 11.304545402526855\n",
            "Epoch [152/200] Batch 0/6                       Loss D: 0.4105647802352905, loss G: 13.894658088684082\n",
            "Epoch [153/200] Batch 0/6                       Loss D: 0.6201491951942444, loss G: 10.66112232208252\n",
            "Epoch [154/200] Batch 0/6                       Loss D: 0.3222496807575226, loss G: 12.54425048828125\n",
            "Epoch [155/200] Batch 0/6                       Loss D: 0.5075486898422241, loss G: 12.454713821411133\n",
            "Epoch [156/200] Batch 0/6                       Loss D: 0.5896839499473572, loss G: 11.913337707519531\n",
            "Epoch [157/200] Batch 0/6                       Loss D: 0.512385368347168, loss G: 13.442800521850586\n",
            "Epoch [158/200] Batch 0/6                       Loss D: 0.5791923403739929, loss G: 11.832088470458984\n",
            "Epoch [159/200] Batch 0/6                       Loss D: 0.5750290155410767, loss G: 12.12375545501709\n",
            "Epoch [160/200] Batch 0/6                       Loss D: 0.37717685103416443, loss G: 12.92930793762207\n",
            "Epoch [161/200] Batch 0/6                       Loss D: 0.5611199140548706, loss G: 11.444219589233398\n",
            "Epoch [162/200] Batch 0/6                       Loss D: 0.4410845637321472, loss G: 11.439228057861328\n",
            "Epoch [163/200] Batch 0/6                       Loss D: 0.6738600134849548, loss G: 12.17855453491211\n",
            "Epoch [164/200] Batch 0/6                       Loss D: 0.35419762134552, loss G: 13.006203651428223\n",
            "Epoch [165/200] Batch 0/6                       Loss D: 0.4820234775543213, loss G: 13.500473976135254\n",
            "Epoch [166/200] Batch 0/6                       Loss D: 0.37647736072540283, loss G: 11.853805541992188\n",
            "Epoch [167/200] Batch 0/6                       Loss D: 0.4311283826828003, loss G: 12.783289909362793\n",
            "Epoch [168/200] Batch 0/6                       Loss D: 0.5597404837608337, loss G: 11.85347843170166\n",
            "Epoch [169/200] Batch 0/6                       Loss D: 0.3806725740432739, loss G: 11.972315788269043\n",
            "Epoch [170/200] Batch 0/6                       Loss D: 0.4257035255432129, loss G: 12.068464279174805\n",
            "Epoch [171/200] Batch 0/6                       Loss D: 0.5660625100135803, loss G: 12.618753433227539\n",
            "Epoch [172/200] Batch 0/6                       Loss D: 0.5542088150978088, loss G: 11.684171676635742\n",
            "Epoch [173/200] Batch 0/6                       Loss D: 0.5385372042655945, loss G: 12.31732177734375\n",
            "Epoch [174/200] Batch 0/6                       Loss D: 0.5401322841644287, loss G: 12.765032768249512\n",
            "Epoch [175/200] Batch 0/6                       Loss D: 0.22979292273521423, loss G: 11.724386215209961\n",
            "Epoch [176/200] Batch 0/6                       Loss D: 0.35102176666259766, loss G: 12.89077377319336\n",
            "Epoch [177/200] Batch 0/6                       Loss D: 0.5433424711227417, loss G: 12.23316478729248\n",
            "Epoch [178/200] Batch 0/6                       Loss D: 0.5242136120796204, loss G: 12.334936141967773\n",
            "Epoch [179/200] Batch 0/6                       Loss D: 0.36438310146331787, loss G: 12.253495216369629\n",
            "Epoch [180/200] Batch 0/6                       Loss D: 0.44742685556411743, loss G: 13.784564018249512\n",
            "Epoch [181/200] Batch 0/6                       Loss D: 0.3588166832923889, loss G: 11.989322662353516\n",
            "Epoch [182/200] Batch 0/6                       Loss D: 0.5386662483215332, loss G: 11.515063285827637\n",
            "Epoch [183/200] Batch 0/6                       Loss D: 0.4709506034851074, loss G: 12.701497077941895\n",
            "Epoch [184/200] Batch 0/6                       Loss D: 0.5292338728904724, loss G: 10.741564750671387\n",
            "Epoch [185/200] Batch 0/6                       Loss D: 0.3910680413246155, loss G: 11.003987312316895\n",
            "Epoch [186/200] Batch 0/6                       Loss D: 0.4773322343826294, loss G: 12.0274076461792\n",
            "Epoch [187/200] Batch 0/6                       Loss D: 0.569014310836792, loss G: 12.20985221862793\n",
            "Epoch [188/200] Batch 0/6                       Loss D: 0.3627559542655945, loss G: 11.277847290039062\n",
            "Epoch [189/200] Batch 0/6                       Loss D: 0.3392435312271118, loss G: 13.02688980102539\n",
            "Epoch [190/200] Batch 0/6                       Loss D: 0.5042358040809631, loss G: 11.609167098999023\n",
            "Epoch [191/200] Batch 0/6                       Loss D: 0.46252816915512085, loss G: 12.853704452514648\n",
            "Epoch [192/200] Batch 0/6                       Loss D: 0.3436117172241211, loss G: 12.211424827575684\n",
            "Epoch [193/200] Batch 0/6                       Loss D: 0.43732792139053345, loss G: 11.603822708129883\n",
            "Epoch [194/200] Batch 0/6                       Loss D: 0.4320371747016907, loss G: 11.987895965576172\n",
            "Epoch [195/200] Batch 0/6                       Loss D: 0.6006056070327759, loss G: 12.293325424194336\n",
            "Epoch [196/200] Batch 0/6                       Loss D: 0.4881265461444855, loss G: 12.660614967346191\n",
            "Epoch [197/200] Batch 0/6                       Loss D: 0.5576604604721069, loss G: 10.856627464294434\n",
            "Epoch [198/200] Batch 0/6                       Loss D: 0.45447033643722534, loss G: 11.595059394836426\n",
            "Epoch [199/200] Batch 0/6                       Loss D: 0.40441036224365234, loss G: 12.630743980407715\n"
          ]
        }
      ]
    }
  ]
}